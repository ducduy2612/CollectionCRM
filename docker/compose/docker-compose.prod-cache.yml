version: '3.8'

# Production Cache & Message Broker Tier
# Deploy this on a dedicated server for Redis and Kafka

x-common-config: &common-config
  restart: unless-stopped
  logging:
    driver: "json-file"
    options:
      max-size: "50m"
      max-file: "5"

services:
  # Redis Primary
  redis-primary:
    image: redis:7-alpine
    <<: *common-config
    command: >
      redis-server
      --maxmemory 2gb
      --maxmemory-policy allkeys-lru
      --appendonly yes
      --appendfilename "redis-prod.aof"
      --appendfsync everysec
      --save "900 1"
      --save "300 10"
      --save "60 10000"
      --requirepass ${REDIS_PASSWORD}
      --tcp-keepalive 300
      --timeout 0
      --tcp-backlog 511
      --databases 16
      --rdbcompression yes
      --rdbchecksum yes
    env_file:
      - ../../config/redis.prod.env
    volumes:
      - redis-prod-data:/data
      - redis-prod-config:/usr/local/etc/redis
    ports:
      - "6379:6379"
    networks:
      - collectioncrm-cache-network
    healthcheck:
      test: ["CMD", "redis-cli", "--pass", "${REDIS_PASSWORD}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G


  # Zookeeper for Kafka
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    <<: *common-config
    hostname: zookeeper
    environment:
      - ZOOKEEPER_SERVER_ID=1
      - ZOOKEEPER_CLIENT_PORT=2181
      - ZOOKEEPER_TICK_TIME=2000
      - ZOOKEEPER_HEAP_OPTS=-Xmx512m -Xms512m
    volumes:
      - zookeeper-data:/var/lib/zookeeper/data
      - zookeeper-logs:/var/lib/zookeeper/log
    ports:
      - "2181:2181"
    networks:
      - collectioncrm-cache-network
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M

  # Kafka Broker
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    <<: *common-config
    hostname: kafka
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_LISTENERS=INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:19092
      - KAFKA_ADVERTISED_LISTENERS=INTERNAL://kafka:9092,EXTERNAL://localhost:19092
      - KAFKA_INTER_BROKER_LISTENER_NAME=INTERNAL
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_TRANSACTION_STATE_LOG_MIN_ISR=1
      - KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1
      - KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS=3000
      - KAFKA_AUTO_CREATE_TOPICS_ENABLE=false
      - KAFKA_DELETE_TOPIC_ENABLE=true
      - KAFKA_LOG_RETENTION_HOURS=168
      - KAFKA_LOG_SEGMENT_BYTES=1073741824
      - KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS=300000
      - KAFKA_HEAP_OPTS=-Xmx1G -Xms1G
    volumes:
      - kafka-data:/var/lib/kafka/data
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "19092:19092"
    networks:
      - collectioncrm-cache-network
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics --bootstrap-server localhost:9092 --list"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G

volumes:
  redis-prod-data:
    driver: local
  redis-prod-config:
    driver: local
  zookeeper-data:
    driver: local
  zookeeper-logs:
    driver: local
  kafka-data:
    driver: local

networks:
  collectioncrm-cache-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.31.0.0/16